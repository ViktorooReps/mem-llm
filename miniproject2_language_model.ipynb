{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Miniproject 2**\n",
    "## **~Large~ Small Language Model**\n",
    "\n",
    "### **Objective**\n",
    "Implement a transformer-based, character-level language model (GPT-like) and train it on the Shakespeare dataset. By the end of this project, you should be able to generate Shakespearean-like text given a seed string.\n",
    "\n",
    "You will probably want to train the model on a GPU. You can use free GPUs on [Google Colab](https://colab.research.google.com/?utm_source=scs-index)."
   ],
   "metadata": {
    "id": "--Cvru1cgwyP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Dataset**:\n",
    "\n",
    "The Shakespeare dataset contains the complete works of William Shakespeare, including his plays, poems, and sonnets.\n",
    "\n",
    "[**Download link**](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)\n",
    "\n",
    "In a character-level language model, each character in the input data is mapped to its respective index from a dictionary. The input to the model is in the form (B, N), where B is the batch size and N is the number of tokens for each sequence. The model was tested with B=N=128, but feel free to explore different values.\n",
    "\n",
    "An interface for the dataset class that takes care of tokenization is provided below.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters.\n",
    "\n",
    "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "\n",
    "        chars = ... # get characters from the input data\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) } # map characters to integer indices\n",
    "\n",
    "        ...\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        # encode every character to an integer\n",
    "        # return the chunk and the shifted version as tensors\n",
    "        pass\n",
    "```\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "f_rT3xwrhieb"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Requirements**\n",
    "\n",
    "#### **Architecture**\n",
    "\n",
    "Implement the Transformer's decoder-only structure.\n",
    "This includes\n",
    "\n",
    "* input token embeddings\n",
    "* the causal multi-head self-attention mechanism\n",
    "* feed-forward neural networks\n",
    "* positional encodings, residual connections, layer normalizations.\n",
    "\n",
    "The project was tested with $12$ layers, $8$ attention heads, and $768$ embedding dimensions, on a single GPU.\n",
    "\n",
    "The `forward` method for the entire model has the following form:\n",
    "\n",
    "```\n",
    "tok_emb = WTE(idx) # token embeddings\n",
    "pos_emb = WPE(pos) # position embeddings\n",
    "x = Dropout(tok_emb + pos_emb)\n",
    "for Block in Blocks:\n",
    "    x = Block(x)\n",
    "x = Final_LayerNorm(x)\n",
    "logits = LM_Head(x)\n",
    "```\n",
    "\n",
    "The `forward` method for the transformer block has the following form:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "x = x + self.CausalSelfAttn(self.LayerNorm_1(x))\n",
    "out = x + self.MLP(self.LayerNorm_2(x))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Training**\n",
    "\n",
    "In a character-level transformer language model, the goal is to predict the next character in a sequence given the previous characters. To train such a model effectively, we use two versions of our data: the input sequence and a shifted version of this sequence, which serves as the target for our predictions.\n",
    "\n",
    "Preprocess the dataset to a character-level representation.\n",
    "Use a sliding window approach for sequence chunks (e.g., window size of $128$ characters).\n",
    "Implement causal masking for the self-attention mechanism.\n",
    "Use the [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer and the cross-entropy loss.\n",
    "\n",
    "**Optional**:\n",
    "\n",
    "* Implement a learning rate decay strategy\n",
    "* Implement gradient clipping\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **Evaluation and Inference**\n",
    "\n",
    "* Monitor the cross-entropy loss. Use a seed string to initialize the model and generate Shakespearean-like text.\n",
    "\n",
    "* In order to generate the characters, at each generation step you can either select the character with the highest probability, or you can sample according to the output distribution.\n",
    "\n",
    "The high-level pseudocode for generation is:\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    context = \"O God, O God!\"\n",
    "    tokenized_context = tokenize(context)\n",
    "    # the model should implement a method to generate tokens given a prompt\n",
    "    y = model.generate(tokenized, ...)\n",
    "    completion = tokens_to_string(y)\n",
    "```\n",
    "\n",
    "**Optional**:\n",
    "* Compute the [perplexity](https://medium.com/@priyankads/perplexity-of-language-models-41160427ed72#:~:text=Intuitively%2C%20perplexity%20means%20to%20be,loss%20obtained%20from%20the%20model.) metric for quantitative evaluation."
   ],
   "metadata": {
    "id": "VV7OAXGRhf_V"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Example Outputs**\n",
    "\n",
    "The following are my outputs after $6000$ steps of training, with the seed string \"O God, O God!\"\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "O God, O God! neither? unto the base very ears,\n",
    "As damned with it.\n",
    "\n",
    "DUKE OF YORK:\n",
    "Away! Once more, one word.\n",
    "\n",
    "RICHARD:\n",
    "Clove, dear so; and therein my son will be\n",
    "false of woe: if ye seems to be the mother\n",
    "Of gracious order this time when R going kinsperse eyes,\n",
    "What dost bewreck her fairer drying tears.\n",
    "\n",
    "NORTHUMBERLAND:\n",
    "Have you forgot the Duke of Norfolk, get him to\n",
    "again; and and agilic: there is my spirit\n",
    "So maly did must such a marble perfection.\n",
    "\n",
    "ELBOW:\n",
    "Come, bring them with oaths, and so deliver\n",
    "```\n"
   ],
   "metadata": {
    "id": "8t88Dcn8JZ8M"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resources:\n",
    "\n",
    "* Vaswani et al., \"Attention is All You Need\": [link](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "* Illustrated Transformer by Jay Alammar: [link](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "* OpenAI GPT-2 Paper: [link](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "* Deep Learning Course slides on transformers: [link](https://fleuret.org/dlc/materials/dlc-handout-13-3-transformers.pdf)"
   ],
   "metadata": {
    "id": "k0SY7CGAhnkp"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dependencies"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:44:51.657078Z",
     "start_time": "2024-11-12T22:44:45.741395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import abc\n",
    "import json\n",
    "import time\n",
    "from abc import abstractmethod\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from nltk.tokenize import PunktTokenizer\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.attention.flex_attention import create_block_mask, and_masks, flex_attention\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from typing import Iterable, TypeVar\n",
    "from itertools import chain\n",
    "\n",
    "from tqdm.auto import tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:14:26.795111Z",
     "start_time": "2024-11-12T22:14:26.691928Z"
    }
   },
   "cell_type": "code",
   "source": "nltk.download('punkt')",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/shcherba/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Implementation"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Some interfaces that will be used:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:31:22.911964Z",
     "start_time": "2024-11-12T22:31:22.907836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_T = TypeVar('_T')\n",
    "\n",
    "\n",
    "class Configurable(metaclass=abc.ABCMeta):\n",
    "    @abstractmethod\n",
    "    def to_config(self):\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls: _T, config: dict) -> _T:\n",
    "        return cls(**config)\n",
    "    \n",
    "    def save(self, path: str | Path) -> None:\n",
    "        config = self.to_config()\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls: _T, path: str | Path) -> _T:\n",
    "        with open(path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        return cls.from_config(config)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenization"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For tokenization, we will just use Python's built-in `ord` function. We will reserve a fixed tokenizer size, 128 is enough for English text. Character NUL (`0`) will be used for unknown tokens. "
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:14:26.803994Z",
     "start_time": "2024-11-12T22:14:26.800833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ords = list(range(128))\n",
    "for o in ords:\n",
    "    print(f'\"{chr(o)}\"'.replace('\\n', '\\\\n').replace('\\t', '\\\\t'), end=' ')\n",
    "    if o and not o % 10:\n",
    "        print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\u0000\" \"\u0001\" \"\u0002\" \"\u0003\" \"\u0004\" \"\u0005\" \"\u0006\" \"\u0007\" \" \"\\t\" \"\\n\" \n",
      "\" \"\u000E\" \"\u000F\" \"\u0010\" \"\u0011\" \"\u0012\" \"\u0013\" \"\u0014\" \n",
      "\"\u0015\" \"\u0016\" \"\u0017\" \"\u0018\" \"\u0019\" \"\u001A\" \"\u001B\" \"\u001C\" \"\u001D\" \"\u001E\" \n",
      "\"\u001F\" \" \" \"!\" \"\"\" \"#\" \"$\" \"%\" \"&\" \"'\" \"(\" \n",
      "\")\" \"*\" \"+\" \",\" \"-\" \".\" \"/\" \"0\" \"1\" \"2\" \n",
      "\"3\" \"4\" \"5\" \"6\" \"7\" \"8\" \"9\" \":\" \";\" \"<\" \n",
      "\"=\" \">\" \"?\" \"@\" \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \n",
      "\"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \n",
      "\"Q\" \"R\" \"S\" \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\" \n",
      "\"[\" \"\\\" \"]\" \"^\" \"_\" \"`\" \"a\" \"b\" \"c\" \"d\" \n",
      "\"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \n",
      "\"o\" \"p\" \"q\" \"r\" \"s\" \"t\" \"u\" \"v\" \"w\" \"x\" \n",
      "\"y\" \"z\" \"{\" \"|\" \"}\" \"~\" \"\" "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The tokenizer will also provide masking mechanism. We will use `FlexAttention` interface to explore sparse masking later, see Section ??. To enable experiments down the line, we will add the possibility of masking out:\n",
    "\n",
    "1) Words, as defined by non-`[a-zA-Z0-9]` characters separation\n",
    "2) Sentences detected by NLTK.\n",
    "\n",
    "The motivation will be provided later, for now the important information is as follows:\n",
    "1. We denote non-`[a-zA-Z0-9]` as special tokens\n",
    "2. We divide the input tokens into \"segments\". Tokens can attend only within their own segments.\n",
    "3. Special tokens that serve as segment separators are grouped into their own segment (so any special can attend to every other special)\n",
    "4. The special token that follows the segment is an exception. It can attend to its segment and to the one segment before.\n",
    "\n",
    "For example, if we segment by sentences:\n",
    "`v`\n",
    "Where `<s1>` and `<s2>` are space characters with name, `<s1>` can attend to any token in `Oh fox!` and `<s2>` can attend to any character in `Why the long face?` and `<s1>`. At the same time, `f` in `Why the long face?` can only attend to `Why the long ` and `<s2>` to preserve causality.\n",
    "\n",
    "Last remarks: we will always add STX, Start-of-TeXt (`2`) token to the start of the sequence to serve as \"sink\" token, see StreamingLLM and other research on the significance of this token. Another special token is ETX, End-of-TeXt (`3`), will be added to the end of complete text (end of document)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:14:26.814093Z",
     "start_time": "2024-11-12T22:14:26.804733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "IS_SPECIAL_REGEX = re.compile(r'[^a-zA-Z0-9]')\n",
    "\n",
    "class Tokenizer(Configurable):\n",
    "    def __init__(\n",
    "            self, \n",
    "            vocab_size: int = 128, \n",
    "            device: str = 'cpu', \n",
    "            segment: str | None = None,  # \"sentence\", \"word\" or \"sentence+word\"\n",
    "    ):\n",
    "        self.size = vocab_size\n",
    "        self.segment = segment if segment is not None else ''\n",
    "        \n",
    "        self.is_special = torch.tensor([\n",
    "            IS_SPECIAL_REGEX.match(f'{chr(o)}') is not None\n",
    "            for o in ords\n",
    "        ], dtype=torch.bool, device=device)\n",
    "        \n",
    "        self.tokenizer_sent = None\n",
    "        \n",
    "        self.segment_sent = False\n",
    "        self.segment_word = False\n",
    "        \n",
    "        self.unk_token = 0\n",
    "        self.bot_token = 2\n",
    "        self.eot_token = 3\n",
    "        self.sep_token = ord(' ')\n",
    "        \n",
    "        for segment_type in self.segment.split('+'):\n",
    "            if segment_type == 'sentence':\n",
    "                self.tokenizer_sent = PunktTokenizer('english')\n",
    "                self.segment_sent = True\n",
    "            if segment_type == 'word':\n",
    "                self.segment_word = True\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.is_special.device\n",
    "    \n",
    "    def to(self, device: str):\n",
    "        self.is_special = self.is_special.to(device)\n",
    "        return self\n",
    "    \n",
    "    def to_config(self):\n",
    "        return {\n",
    "            'size': self.size,\n",
    "            'segment': self.segment,\n",
    "        }\n",
    "        \n",
    "    def encode(self, text: str, *, add_sink: bool = True) -> dict:\n",
    "        if self.segment_sent:\n",
    "            segments = self.tokenizer_sent.span_tokenize(text)\n",
    "        else:\n",
    "            segments = [(0, len(text))]\n",
    "        \n",
    "        tokens = list(map(ord, text))\n",
    "        \n",
    "        token_segments = []\n",
    "        segment_mask = []\n",
    "        segment_starts = []\n",
    "        \n",
    "        prev_segm = None\n",
    "        for segm in segments: \n",
    "            start, end = segm\n",
    "            \n",
    "            if prev_segm is None:\n",
    "                if add_sink:\n",
    "                    # add sink\n",
    "                    token_segments.append([self.bot_token])\n",
    "                    segment_mask.append([False])\n",
    "            elif prev_segm[1] == start:\n",
    "                # separate the sentences    \n",
    "                token_segments.append([self.sep_token])\n",
    "                segment_mask.append([False])\n",
    "            else:\n",
    "                prev_start, prev_end = prev_segm\n",
    "                \n",
    "                # everything between the sentences\n",
    "                token_segments.append(tokens[prev_end:start])\n",
    "                segment_mask.append([False] * (start - prev_end))\n",
    "            \n",
    "            token_segments.append(tokens[start:end])\n",
    "            segment_mask.append([True] * (end - start))\n",
    "            segment_starts.append(start)\n",
    "            prev_segm = segm\n",
    "        \n",
    "        final_s, final_e = prev_segm\n",
    "        if final_e != len(tokens):\n",
    "            token_segments.append(tokens[final_e:len(tokens)])\n",
    "            segment_mask.append([False] * (len(tokens) - final_e))\n",
    "            \n",
    "        all_tokens = chain.from_iterable(token_segments)\n",
    "        all_segment_mask = chain.from_iterable(segment_mask)\n",
    "        \n",
    "        tokens_torch = torch.tensor(list(all_tokens), device=self.device, dtype=torch.int)\n",
    "        tokens_torch[tokens_torch >= self.size] = self.unk_token\n",
    "        \n",
    "        return {\n",
    "            'tokens': tokens_torch,\n",
    "            'segment_mask': torch.tensor(list(all_segment_mask), device=self.device, dtype=torch.bool),\n",
    "            'specials_mask': self.is_special[tokens_torch],\n",
    "            'segment_starts': torch.tensor(segment_starts, device=self.device, dtype=torch.int),\n",
    "        }\n",
    "        \n",
    "    def decode(self, tokens: Iterable[int]) -> str:\n",
    "        # skip sink and pad tokens\n",
    "        return ''.join(chr(o) for o in tokens if o != self.bot_token)\n",
    "        \n",
    "\n",
    "IS_SPECIAL_REGEX.match(f'{chr(33)}'), chr(33), IS_SPECIAL_REGEX.match(f'{chr(49)}'), chr(49)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<re.Match object; span=(0, 1), match='!'>, '!', None, '1')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:14:26.819397Z",
     "start_time": "2024-11-12T22:14:26.814943Z"
    }
   },
   "cell_type": "code",
   "source": "Tokenizer().is_special",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        False, False, False, False, False, False, False, False,  True,  True,\n",
       "         True,  True,  True,  True,  True, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:14:26.836757Z",
     "start_time": "2024-11-12T22:14:26.820078Z"
    }
   },
   "cell_type": "code",
   "source": "Tokenizer(segment='sentence').encode('I am a token. You are a token...........  So what?')",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([  2,  73,  32,  97, 109,  32,  97,  32, 116, 111, 107, 101, 110,  46,\n",
       "          32,  89, 111, 117,  32,  97, 114, 101,  32,  97,  32, 116, 111, 107,\n",
       "         101, 110,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  32,\n",
       "          32,  83, 111,  32, 119, 104,  97, 116,  63], dtype=torch.int32),\n",
       " 'segment_mask': tensor([False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True]),\n",
       " 'specials_mask': tensor([ True, False,  True, False, False,  True, False,  True, False, False,\n",
       "         False, False, False,  True,  True, False, False, False,  True, False,\n",
       "         False, False,  True, False,  True, False, False, False, False, False,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True, False, False,  True, False, False, False, False,\n",
       "          True]),\n",
       " 'segment_starts': tensor([ 0, 14, 42], dtype=torch.int32)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pretokenize the datasets"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Shakespear"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:14:26.947407Z",
     "start_time": "2024-11-12T22:14:26.837540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "download_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "response = requests.get(download_url)\n",
    "\n",
    "file_path = 'tinyshakespeare_input.txt'\n",
    "with open(file_path, 'wb') as file:\n",
    "    file.write(response.content)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:14:27.322189Z",
     "start_time": "2024-11-12T22:14:26.948761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "tokenizer = Tokenizer(segment='sentence')\n",
    "tokenized = tokenizer.encode(content, add_sink=False)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:14:27.327941Z",
     "start_time": "2024-11-12T22:14:27.323670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_path = Path('data')\n",
    "data_path.mkdir(exist_ok=True)\n",
    "\n",
    "path = data_path / 'tinyshakespeare'\n",
    "path.mkdir(exist_ok=True)\n",
    "\n",
    "np_tokens = tokenized['tokens'].numpy().astype(np.uint8)\n",
    "np_segment_mask = tokenized['segment_mask'].numpy().astype(bool)\n",
    "np_special_mask = tokenized['specials_mask'].numpy().astype(bool)\n",
    "np_segment_starts = tokenized['segment_starts'].numpy().astype(int)\n",
    "\n",
    "len(np_tokens)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:14:27.330778Z",
     "start_time": "2024-11-12T22:14:27.328616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_sent = len(np_segment_starts)\n",
    "n_train_sent = int(n_sent * 0.95)\n",
    "n_val_sent = n_sent - n_train_sent\n",
    "n_train_sent, n_val_sent"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11837, 623)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:14:27.333967Z",
     "start_time": "2024-11-12T22:14:27.331360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np_segment_ends = np.roll(np_segment_starts, -1)\n",
    "np_segment_ends[-1] = len(np_tokens)\n",
    "segm_len = np_segment_ends - np_segment_starts\n",
    "\n",
    "n_train_tokens = int(segm_len[:n_train_sent].sum())\n",
    "n_val_tokens = len(np_tokens) - n_train_tokens\n",
    "n_train_tokens, n_val_tokens"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1070080, 45314)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:14:27.390864Z",
     "start_time": "2024-11-12T22:14:27.334594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_token_mask = torch.concat([\n",
    "    torch.ones(n_train_tokens, dtype=torch.bool), \n",
    "    torch.zeros(n_val_tokens, dtype=torch.bool)\n",
    "])\n",
    "val_token_mask = ~train_token_mask\n",
    "\n",
    "train_sent_mask = torch.concat([\n",
    "    torch.ones(n_train_sent, dtype=torch.bool), \n",
    "    torch.zeros(n_val_sent, dtype=torch.bool)\n",
    "])\n",
    "val_sent_mask = ~train_sent_mask\n",
    "\n",
    "np_tokens_train = np_tokens[train_token_mask]\n",
    "np_segment_mask_train = np_segment_mask[train_token_mask]\n",
    "np_special_mask_train = np_special_mask[train_token_mask]\n",
    "np_segment_starts_train = np_segment_starts[train_sent_mask]\n",
    "\n",
    "np_tokens_val = np_tokens[val_token_mask]\n",
    "np_segment_mask_val = np_segment_mask[val_token_mask]\n",
    "np_special_mask_val = np_special_mask[val_token_mask]\n",
    "np_segment_starts_val = np_segment_starts[val_sent_mask]"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:14:27.747070Z",
     "start_time": "2024-11-12T22:14:27.705453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_path = path / 'train'\n",
    "train_path.mkdir(exist_ok=True)\n",
    "\n",
    "np.save(train_path / 'tokens.npy', np_tokens_train)\n",
    "np.save(train_path / 'segment_mask.npy', np_segment_mask_train)\n",
    "np.save(train_path / 'special_mask.npy', np_special_mask_train)\n",
    "np.save(train_path / 'segment_starts.npy', np_segment_starts_train)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:14:28.049427Z",
     "start_time": "2024-11-12T22:14:28.022856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val_path = path / 'val'\n",
    "val_path.mkdir(exist_ok=True)\n",
    "\n",
    "np.save(val_path / 'tokens.npy', np_tokens_val)\n",
    "np.save(val_path / 'segment_mask.npy', np_segment_mask_val)\n",
    "np.save(val_path / 'special_mask.npy', np_special_mask_val)\n",
    "np.save(val_path / 'segment_starts.npy', np_segment_starts_val)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## FineWeb Edu"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:14:28.802148Z",
     "start_time": "2024-11-12T22:14:28.799857Z"
    }
   },
   "cell_type": "code",
   "source": "# TODO",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pretokenized dataset loader"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:44:59.086659Z",
     "start_time": "2024-11-12T22:44:59.053722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path: str | Path, *, start_token: int, target_length: int):\n",
    "        path = Path(path)\n",
    "        assert path.exists() and path.is_dir()\n",
    "        \n",
    "        tokens_path = path / 'tokens.npy'\n",
    "        segment_mask = path / 'segment_mask.npy'\n",
    "        special_mask = path / 'special_mask.npy'\n",
    "        segment_starts = path / 'segment_starts.npy'\n",
    "        \n",
    "        self.start_token = start_token\n",
    "        self.target_length = target_length\n",
    "        \n",
    "        self.tokens = np.memmap(tokens_path, dtype=np.uint8, mode='r')\n",
    "        self.segment_mask = np.memmap(segment_mask, dtype=np.uint8, mode='r')\n",
    "        self.special_mask = np.memmap(special_mask, dtype=np.uint8, mode='r')\n",
    "        self.segment_starts = np.load(segment_starts)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.segment_starts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = self.segment_starts[idx]\n",
    "        end_idx = start_idx + self.target_length - 1  # reserve one token for start_token\n",
    "        \n",
    "        tokens = torch.tensor(self.tokens[start_idx:end_idx].astype(int))\n",
    "        segment_mask = torch.tensor(self.segment_mask[start_idx:end_idx])\n",
    "        special_mask = torch.tensor(self.special_mask[start_idx:end_idx])\n",
    "        \n",
    "        return {\n",
    "            'tokens': torch.concatenate([torch.tensor([self.start_token]), tokens]),\n",
    "            'segment_mask': torch.concatenate([torch.tensor([False]), segment_mask]),\n",
    "            'special_mask': torch.concatenate([torch.tensor([True]), special_mask])\n",
    "        }\n",
    "        \n",
    "Dataset('data/tinyshakespeare/train', start_token=2, target_length=10)[100]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([  2, 100,  32, 109,  97, 107, 101,  32,  98, 111]),\n",
       " 'segment_mask': tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8),\n",
       " 'special_mask': tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0], dtype=torch.uint8)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# FlexAttention masking and Collator"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:45:22.080169Z",
     "start_time": "2024-11-12T22:45:22.066578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_intervals(boundary_mask):\n",
    "    # Find cumulative sum of the gaps to identify different intervals\n",
    "    interval_idx = torch.cumsum(boundary_mask, dim=-1) + 1\n",
    "    shifted = interval_idx.roll(1)\n",
    "    shifted[:, 0] = interval_idx[:, 0]\n",
    "    \n",
    "    return shifted\n",
    "\n",
    "\n",
    "def get_interval_block_mask(interval_mask, *, max_distance: int = 0):\n",
    "    boundary_mask = ~interval_mask\n",
    "    interval_idx = calculate_intervals(boundary_mask)\n",
    "    \n",
    "    def interval_block_mask(b, h, q_idx, kv_idx):\n",
    "        # can only attend to tokens in the same interval\n",
    "        # or boundaries between intervals\n",
    "        q_interval_id = interval_idx[b, q_idx]\n",
    "        kv_interval_id = interval_idx[b, kv_idx]\n",
    "        return boundary_mask[b, kv_idx] | (q_idx - kv_idx < max_distance) | torch.eq(q_interval_id, kv_interval_id)\n",
    "    \n",
    "    return interval_block_mask\n",
    "\n",
    "\n",
    "def get_padding_block_mask(tokens, *, padding_token):\n",
    "    \n",
    "    def padding_block_mask(b, h, q_idx, kv_idx):\n",
    "        # nothing can attend to padding tokens and padding tokens cannot attend to anything\n",
    "        q_token = tokens[b, q_idx]\n",
    "        kv_token = tokens[b, kv_idx]\n",
    "        return torch.ne(q_token, padding_token) & torch.ne(kv_token, padding_token)\n",
    "    \n",
    "    return padding_block_mask\n",
    "\n",
    "\n",
    "def causal_block_mask(b, h, q_idx, kv_idx):\n",
    "    # can only attend to past tokens and self\n",
    "    return q_idx >= kv_idx\n",
    "\n",
    "\n",
    "class Collator:\n",
    "    def __init__(\n",
    "            self, \n",
    "            padding_token: int, \n",
    "            mask_words: bool = False, \n",
    "            mask_segments: bool = False,\n",
    "            device: str = 'cpu'\n",
    "    ):\n",
    "        self.padding_token = padding_token\n",
    "        self.mask_words = mask_words\n",
    "        self.mask_segments = mask_segments\n",
    "        self.device = device\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        masks = [causal_block_mask]\n",
    "        \n",
    "        # Extract each element type in the batch\n",
    "        tokens = [item['tokens'].to(self.device) for item in batch]\n",
    "        \n",
    "        if self.mask_segments:\n",
    "            segment_mask = [item['segment_mask'] for item in batch]\n",
    "            segment_mask_padded = pad_sequence(segment_mask, batch_first=True, padding_value=0)\n",
    "            masks.append(get_interval_block_mask(segment_mask_padded))\n",
    "        \n",
    "        if self.mask_words:\n",
    "            specials_mask = [item['specials_mask'] for item in batch]\n",
    "            specials_mask_padded = pad_sequence(specials_mask, batch_first=True, padding_value=0)\n",
    "            masks.append(get_interval_block_mask(~specials_mask_padded))\n",
    "        \n",
    "        # Pad tokens with the specified padding token index\n",
    "        tokens_padded = pad_sequence(tokens, batch_first=True, padding_value=self.padding_token)\n",
    "        # masks.append(get_padding_block_mask(tokens_padded, padding_token=self.padding_token))\n",
    "        \n",
    "        # Determine batch and sequence dimensions\n",
    "        batch_size, seq_length = tokens_padded.size()\n",
    "        \n",
    "        # Create causal block mask using FlexAttention's create_block_mask function\n",
    "        block_mask = create_block_mask(\n",
    "            mask_mod=and_masks(*masks),  # Causal mask function\n",
    "            B=batch_size,\n",
    "            H=None,                        # Number of heads; adjust if using multi-head attention\n",
    "            Q_LEN=seq_length,\n",
    "            KV_LEN=seq_length,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # Create final batch dictionary\n",
    "        batch_dict = {\n",
    "            'tokens': tokens_padded,\n",
    "            'block_mask': block_mask   # Add block mask to the batch\n",
    "        }\n",
    "        \n",
    "        return batch_dict\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:35:23.192232Z",
     "start_time": "2024-11-12T22:35:23.144100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def visualize_mask(mask, text, title=\"\"):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    ax = sns.heatmap(\n",
    "        mask.numpy(), \n",
    "        cmap=\"Blues\", \n",
    "        cbar=False, \n",
    "        annot=False, \n",
    "        xticklabels=[c for c in [' ', *text]], \n",
    "        yticklabels=[c for c in [' ', *text]],\n",
    "    )\n",
    "    plt.yticks(rotation=270)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Key Tokens\")\n",
    "    plt.ylabel(\"Query Tokens\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "text = \"Oh fox! Why the long face? - To bite you! - Oh no, please don't bite me, I beg you please! - Ok\"\n",
    "\n",
    "# visualize different combinations of masks\n",
    "# 1) only causal\n",
    "# 2) with word masking\n",
    "# 3) with segment masking\n",
    "# 4) with both word and segment masking\n",
    "tokenizer_simple = Tokenizer()\n",
    "tokenizer_sentence = Tokenizer(segment='sentence')\n",
    "\n",
    "tokens = tokenizer_simple.encode(text)['tokens'].unsqueeze(0)\n",
    "segment_mask_word = ~tokenizer_simple.encode(text)['specials_mask'].unsqueeze(0)\n",
    "segment_mask_sentence = tokenizer_sentence.encode(text)['segment_mask'].unsqueeze(0)\n",
    "seq_length = len(tokens[0])\n",
    "\n",
    "# 1) Only causal mask\n",
    "block_mask_causal = torch.zeros((seq_length, seq_length), dtype=torch.bool)\n",
    "for q in range(seq_length):\n",
    "    for kv in range(seq_length):\n",
    "        block_mask_causal[q, kv] = causal_block_mask(0, 0, q, kv)\n",
    "visualize_mask(block_mask_causal, text, title=\"Only Causal Mask\")\n",
    "\n",
    "# 2) Causal + Word Mask\n",
    "interval_mask_word = get_interval_block_mask(segment_mask_word)\n",
    "block_mask_word = torch.zeros((seq_length, seq_length), dtype=torch.bool)\n",
    "for q in range(seq_length):\n",
    "    for kv in range(seq_length):\n",
    "        causal = causal_block_mask(0, 0, q, kv)\n",
    "        interval = interval_mask_word(0, 0, q, kv)\n",
    "        block_mask_word[q, kv] = causal & interval\n",
    "visualize_mask(block_mask_word, text, title=\"Causal + Word Mask\")\n",
    "\n",
    "# 2) Causal + Word Mask + Segment\n",
    "interval_mask_segment = get_interval_block_mask(segment_mask_sentence)\n",
    "block_mask_word = torch.zeros((seq_length, seq_length), dtype=torch.bool)\n",
    "for q in range(seq_length):\n",
    "    for kv in range(seq_length):\n",
    "        causal = causal_block_mask(0, 0, q, kv)\n",
    "        interval = interval_mask_word(0, 0, q, kv)\n",
    "        segment = interval_mask_segment(0, 0, q, kv)\n",
    "        block_mask_word[q, kv] = causal & interval & segment\n",
    "visualize_mask(block_mask_word, text, title=\"Causal + Word Mask + Segment\")\n",
    "\n",
    "# 2) Causal + Word Mask + Segment with max distance cutoff\n",
    "interval_mask_word = get_interval_block_mask(segment_mask_word, max_distance=5)\n",
    "interval_mask_segment = get_interval_block_mask(segment_mask_sentence, max_distance=10)\n",
    "block_mask_word = torch.zeros((seq_length, seq_length), dtype=torch.bool)\n",
    "for q in range(seq_length):\n",
    "    for kv in range(seq_length):\n",
    "        causal = causal_block_mask(0, 0, q, kv)\n",
    "        interval = interval_mask_word(0, 0, q, kv)\n",
    "        segment = interval_mask_segment(0, 0, q, kv)\n",
    "        block_mask_word[q, kv] = causal & interval & segment\n",
    "visualize_mask(block_mask_word, text, title=\"Causal + Word Mask (md 5) + Segment (md 10) with max distance cutoff (md)\")"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 26\u001B[0m\n\u001B[1;32m     19\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOh fox! Why the long face? - To bite you! - Oh no, please don\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt bite me, I beg you please! - Ok\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# visualize different combinations of masks\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# 1) only causal\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# 2) with word masking\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# 3) with segment masking\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# 4) with both word and segment masking\u001B[39;00m\n\u001B[0;32m---> 26\u001B[0m tokenizer_simple \u001B[38;5;241m=\u001B[39m \u001B[43mTokenizer\u001B[49m()\n\u001B[1;32m     27\u001B[0m tokenizer_sentence \u001B[38;5;241m=\u001B[39m Tokenizer(segment\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentence\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     29\u001B[0m tokens \u001B[38;5;241m=\u001B[39m tokenizer_simple\u001B[38;5;241m.\u001B[39mencode(text)[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokens\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Performance comparison depending on the sparsity level (H100 GPU, Shakespear texts)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "By length"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:45:27.064067Z",
     "start_time": "2024-11-12T22:45:25.841691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_trials = 10\n",
    "hidden_dim = 1024\n",
    "\n",
    "class DummyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(128, 128 * 16)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        B, S = x.shape\n",
    "        hidden = self.emb(x)\n",
    "        hidden = hidden.view(B, S, 16, 128).transpose(1, 2)\n",
    "        return flex_attention(hidden, hidden, hidden, block_mask=mask.to(hidden.device))\n",
    "\n",
    "model = DummyModel().cuda()\n",
    "model = torch.compile(model)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:45:31.020303Z",
     "start_time": "2024-11-12T22:45:27.682743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base = 128\n",
    "mult_min = 2\n",
    "mult_max = 100\n",
    "\n",
    "data = []\n",
    "for length_mult in tqdm(range(mult_min, mult_max), total=mult_max - mult_min):\n",
    "    length = base * length_mult\n",
    "    dataset = Dataset('data/tinyshakespeare/train', start_token=2, target_length=length)\n",
    "    \n",
    "    # Different configurations\n",
    "    for name, mask_words, mask_segments in [\n",
    "        ('normal', False, False),\n",
    "        #('mask_words', True, False),\n",
    "        #('mask_words_segments', True, True)\n",
    "    ]:\n",
    "        collator = Collator(padding_token=2, mask_words=mask_words, mask_segments=mask_segments, device='cuda')\n",
    "        loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=collator)\n",
    "        \n",
    "        # Record processing time and memory usage for each batch\n",
    "        for batch in loader:\n",
    "            # Move data to CUDA if necessary\n",
    "            tokens = batch['tokens'].to('cuda')\n",
    "            block_mask = batch['block_mask'].to('cuda')\n",
    "            # warmup\n",
    "            model(tokens, block_mask)\n",
    "\n",
    "            # Clear previous CUDA memory cache\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Measure time and memory\n",
    "            start_time = time.time()\n",
    "            start_memory = torch.cuda.memory_allocated()\n",
    "            \n",
    "            # Process the batch with the model\n",
    "            model(tokens, block_mask)\n",
    "            \n",
    "            # Calculate time taken and memory used\n",
    "            time_taken = time.time() - start_time\n",
    "            end_memory = torch.cuda.memory_allocated()\n",
    "            memory_used = end_memory - start_memory\n",
    "            \n",
    "            # Record results\n",
    "            data.append({\n",
    "                'model': name,\n",
    "                'sequence_length': length,\n",
    "                'time_taken': time_taken,\n",
    "                'memory_used': memory_used\n",
    "            })"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f280f78fa1f44306a4a0c7ca176e3781"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unknown:0: unknown: block: [2,0,0], thread: [32,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [33,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [34,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [35,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [36,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [37,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [38,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [39,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [40,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [41,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [42,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [43,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [44,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [45,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [46,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [47,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [48,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [49,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [50,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [51,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [52,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [53,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [54,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [55,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [56,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [57,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [58,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [59,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [60,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [61,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [62,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [63,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [96,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [97,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [98,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [99,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [100,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [101,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [102,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [103,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [104,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [105,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [106,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [107,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [108,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [109,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [110,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [111,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [112,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [113,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [114,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [115,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [116,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [117,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [118,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [119,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [120,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [121,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [122,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [123,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [124,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [125,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [126,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [127,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [0,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [1,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [2,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [3,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [4,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [5,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [6,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [7,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [8,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [9,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [10,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [11,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [12,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [13,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [14,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [15,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [16,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [17,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [18,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [19,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [20,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [21,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [22,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [23,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [24,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [25,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [26,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [27,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [28,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [29,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [30,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [31,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [0,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [1,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [2,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [3,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [4,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [5,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [6,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [7,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [8,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [9,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [10,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [11,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [12,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [13,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [14,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [15,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [16,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [17,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [18,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [19,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [20,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [21,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [22,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [23,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [24,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [25,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [26,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [27,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [28,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [29,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [30,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [31,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [96,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [97,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [98,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [99,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [100,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [101,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [102,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [103,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [104,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [105,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [106,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [107,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [108,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [109,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [110,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [111,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [112,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [113,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [114,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [115,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [116,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [117,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [118,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [119,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [120,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [121,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [122,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [123,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [124,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [125,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [126,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [127,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [32,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [33,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [34,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [35,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [36,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [37,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [38,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [39,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [40,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [41,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [42,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [43,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [44,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [45,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [46,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [47,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [48,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [49,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [50,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [51,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [52,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [53,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [54,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [55,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [56,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [57,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [58,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [59,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [60,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [61,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [62,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [63,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [64,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [65,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [66,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [67,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [68,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [69,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [70,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [71,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [72,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [73,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [74,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [75,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [76,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [77,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [78,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [79,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [80,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [81,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [82,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [83,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [84,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [85,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [86,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [87,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [88,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [89,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [90,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [91,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [92,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [93,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [94,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [2,0,0], thread: [95,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [64,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [65,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [66,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [67,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [68,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [69,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [70,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [71,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [72,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [73,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [74,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [75,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [76,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [77,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [78,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [79,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [80,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [81,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [82,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [83,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [84,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [85,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [86,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [87,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [88,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [89,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [90,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [91,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [92,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [93,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [94,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n",
      "unknown:0: unknown: block: [3,0,0], thread: [95,0,0] Assertion `index out of bounds: 0 <= tmp4 < 128` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 25\u001B[0m\n\u001B[1;32m     23\u001B[0m block_mask \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mblock_mask\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# warmup\u001B[39;00m\n\u001B[0;32m---> 25\u001B[0m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mblock_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# Clear previous CUDA memory cache\u001B[39;00m\n\u001B[1;32m     28\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1740\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1738\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1739\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1740\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1748\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1750\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1753\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1754\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:556\u001B[0m, in \u001B[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    551\u001B[0m saved_dynamic_layer_stack_depth \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    552\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_functorch\u001B[38;5;241m.\u001B[39mget_dynamic_layer_stack_depth()\n\u001B[1;32m    553\u001B[0m )\n\u001B[1;32m    555\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 556\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    557\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    558\u001B[0m     \u001B[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001B[39;00m\n\u001B[1;32m    559\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_functorch\u001B[38;5;241m.\u001B[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001B[1;32m    560\u001B[0m         saved_dynamic_layer_stack_depth\n\u001B[1;32m    561\u001B[0m     )\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1740\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1738\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1739\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1740\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1748\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1750\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1753\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1754\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[4], line 9\u001B[0m, in \u001B[0;36mDummyModel.forward\u001B[0;34m(self, x, mask)\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39memb \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mEmbedding(\u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m128\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m16\u001B[39m)\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, mask):\n\u001B[1;32m     10\u001B[0m     B, S \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape\n\u001B[1;32m     11\u001B[0m     hidden \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39memb(x)\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:721\u001B[0m, in \u001B[0;36mDisableContext.__call__.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    719\u001B[0m prior \u001B[38;5;241m=\u001B[39m _maybe_set_eval_frame(_callback_from_stance(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback))\n\u001B[1;32m    720\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 721\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    722\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    723\u001B[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1132\u001B[0m, in \u001B[0;36maot_module_simplified.<locals>.forward\u001B[0;34m(*runtime_args)\u001B[0m\n\u001B[1;32m   1130\u001B[0m full_args\u001B[38;5;241m.\u001B[39mextend(params_flat)\n\u001B[1;32m   1131\u001B[0m full_args\u001B[38;5;241m.\u001B[39mextend(runtime_args)\n\u001B[0;32m-> 1132\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompiled_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfull_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:311\u001B[0m, in \u001B[0;36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m    305\u001B[0m     \u001B[38;5;66;03m# It's possible to have trace_joint inside user specified with no_grad() region,\u001B[39;00m\n\u001B[1;32m    306\u001B[0m     \u001B[38;5;66;03m# if there is a nested with enable_grad(), that forces some outputs to require gradients.\u001B[39;00m\n\u001B[1;32m    307\u001B[0m     \u001B[38;5;66;03m# Therefore, we unconditionally turn on enable_grad() for compiled_fn execution.\u001B[39;00m\n\u001B[1;32m    308\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39m_force_original_view_tracking(\n\u001B[1;32m    309\u001B[0m         \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    310\u001B[0m     ), torch\u001B[38;5;241m.\u001B[39menable_grad():\n\u001B[0;32m--> 311\u001B[0m         all_outs \u001B[38;5;241m=\u001B[39m \u001B[43mcall_func_at_runtime_with_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    312\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompiled_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdisable_amp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable_amp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msteal_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[1;32m    313\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    315\u001B[0m     \u001B[38;5;66;03m# When we have an inference graph, we run with grad disabled.\u001B[39;00m\n\u001B[1;32m    316\u001B[0m     \u001B[38;5;66;03m# It's possible to get an inference graph with inputs that require grad,\u001B[39;00m\n\u001B[1;32m    317\u001B[0m     \u001B[38;5;66;03m# in which case we want to make sure autograd is disabled\u001B[39;00m\n\u001B[1;32m    318\u001B[0m     \u001B[38;5;66;03m# (since e.g., inductor will generate aten.addmm.out calls which autograd will complain on)\u001B[39;00m\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;66;03m# NOTE: We use _set_grad_enabled directly to reduce runtime overhead\u001B[39;00m\n\u001B[1;32m    320\u001B[0m     grad_enabled \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mis_grad_enabled()\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001B[0m, in \u001B[0;36mcall_func_at_runtime_with_args\u001B[0;34m(f, args, steal_args, disable_amp)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(f, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_boxed_call\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 126\u001B[0m         out \u001B[38;5;241m=\u001B[39m normalize_as_list(\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    127\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    128\u001B[0m         \u001B[38;5;66;03m# TODO: Please remove soon\u001B[39;00m\n\u001B[1;32m    129\u001B[0m         \u001B[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001B[39;00m\n\u001B[1;32m    130\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    131\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt take boxed arguments. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    132\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    133\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    134\u001B[0m         )\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py:100\u001B[0m, in \u001B[0;36mmake_boxed_func.<locals>.g\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mg\u001B[39m(args):\n\u001B[0;32m--> 100\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/autograd/function.py:575\u001B[0m, in \u001B[0;36mFunction.apply\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m    572\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n\u001B[1;32m    573\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[1;32m    574\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n\u001B[0;32m--> 575\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m    577\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_setup_ctx_defined:\n\u001B[1;32m    578\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    579\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    580\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    581\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    582\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    583\u001B[0m     )\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1582\u001B[0m, in \u001B[0;36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.forward\u001B[0;34m(ctx, *deduped_flat_tensor_args)\u001B[0m\n\u001B[1;32m   1573\u001B[0m     ctx\u001B[38;5;241m.\u001B[39m_compiled_autograd_backward_state \u001B[38;5;241m=\u001B[39m bw_state\n\u001B[1;32m   1575\u001B[0m \u001B[38;5;66;03m# There is a pretty complicated calling convention around what the compiled fw returns.\u001B[39;00m\n\u001B[1;32m   1576\u001B[0m \u001B[38;5;66;03m# The full list of outputs and their relative order is:\u001B[39;00m\n\u001B[1;32m   1577\u001B[0m \u001B[38;5;66;03m# (*tokens, *mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1580\u001B[0m \u001B[38;5;66;03m# - Note that donated buffer logic requires (*saved_tensors, *saved_symints) showing up last\u001B[39;00m\n\u001B[1;32m   1581\u001B[0m \u001B[38;5;66;03m#   in the fw output order.\u001B[39;00m\n\u001B[0;32m-> 1582\u001B[0m fw_outs \u001B[38;5;241m=\u001B[39m \u001B[43mcall_func_at_runtime_with_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1583\u001B[0m \u001B[43m    \u001B[49m\u001B[43mCompiledFunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompiled_fw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1584\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1585\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdisable_amp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable_amp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1586\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1588\u001B[0m num_outputs \u001B[38;5;241m=\u001B[39m CompiledFunction\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mnum_outputs\n\u001B[1;32m   1589\u001B[0m num_outputs_aliased \u001B[38;5;241m=\u001B[39m CompiledFunction\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mnum_outputs_aliased\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001B[0m, in \u001B[0;36mcall_func_at_runtime_with_args\u001B[0;34m(f, args, steal_args, disable_amp)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(f, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_boxed_call\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 126\u001B[0m         out \u001B[38;5;241m=\u001B[39m normalize_as_list(\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    127\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    128\u001B[0m         \u001B[38;5;66;03m# TODO: Please remove soon\u001B[39;00m\n\u001B[1;32m    129\u001B[0m         \u001B[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001B[39;00m\n\u001B[1;32m    130\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    131\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt take boxed arguments. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    132\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    133\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    134\u001B[0m         )\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:491\u001B[0m, in \u001B[0;36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001B[0;34m(runtime_args)\u001B[0m\n\u001B[1;32m    484\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_functionalized_rng_runtime_epilogue(\n\u001B[1;32m    485\u001B[0m         runtime_metadata,\n\u001B[1;32m    486\u001B[0m         out,\n\u001B[1;32m    487\u001B[0m         \u001B[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001B[39;00m\n\u001B[1;32m    488\u001B[0m         runtime_metadata\u001B[38;5;241m.\u001B[39mnum_forward_returns,\n\u001B[1;32m    489\u001B[0m     )\n\u001B[1;32m    490\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[0;32m--> 491\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompiled_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mruntime_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:673\u001B[0m, in \u001B[0;36mEffectTokensWrapper.post_compile.<locals>.inner_fn\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m    670\u001B[0m     args \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m*\u001B[39m([\u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m*\u001B[39m num_tokens), \u001B[38;5;241m*\u001B[39margs]\n\u001B[1;32m    671\u001B[0m     old_args\u001B[38;5;241m.\u001B[39mclear()\n\u001B[0;32m--> 673\u001B[0m outs \u001B[38;5;241m=\u001B[39m \u001B[43mcompiled_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    675\u001B[0m \u001B[38;5;66;03m# Inductor cache DummyModule can return None\u001B[39;00m\n\u001B[1;32m    676\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m outs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/_inductor/codecache.py:1686\u001B[0m, in \u001B[0;36mCompiledFxGraph.__call__\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m   1684\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_callable \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1685\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1686\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcurrent_callable\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1687\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1688\u001B[0m     AutotuneCacheBundler\u001B[38;5;241m.\u001B[39mend_compile()\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/_inductor/utils.py:2065\u001B[0m, in \u001B[0;36malign_inputs_from_check_idxs.<locals>.run\u001B[0;34m(new_inputs)\u001B[0m\n\u001B[1;32m   2063\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun\u001B[39m(new_inputs: List[InputType]):\n\u001B[1;32m   2064\u001B[0m     copy_misaligned_inputs(new_inputs, inputs_to_check)\n\u001B[0;32m-> 2065\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/tmp/torchinductor_shcherba/77/c77eet53igrglg2bpoe73o4n23wa4clpr5twft32buzrhy4nzak4.py:97\u001B[0m, in \u001B[0;36mcall\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m     95\u001B[0m     \u001B[38;5;66;03m# Topologically Sorted Source Nodes: [hidden], Original ATen: [aten.embedding]\u001B[39;00m\n\u001B[1;32m     96\u001B[0m     stream0 \u001B[38;5;241m=\u001B[39m get_raw_stream(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m---> 97\u001B[0m     \u001B[43mtriton_poi_fused_embedding_0\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprimals_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprimals_2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuf0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m524288\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrid\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m524288\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream0\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m primals_2\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (reinterpret_tensor(buf0, (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m16\u001B[39m, \u001B[38;5;241m256\u001B[39m, \u001B[38;5;241m128\u001B[39m), (\u001B[38;5;241m524288\u001B[39m, \u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m2048\u001B[39m, \u001B[38;5;241m1\u001B[39m), \u001B[38;5;241m0\u001B[39m), primals_1, )\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/_inductor/runtime/triton_heuristics.py:986\u001B[0m, in \u001B[0;36mCachingAutotuner.run\u001B[0;34m(self, grid, stream, benchmark_run, *args, **kwargs)\u001B[0m\n\u001B[1;32m    984\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecompile_time_taken_ns \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime_ns() \u001B[38;5;241m-\u001B[39m start_time\n\u001B[1;32m    985\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlaunchers) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 986\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautotune_to_one_config\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrid\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    988\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\n\u001B[1;32m    989\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlaunchers[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mconfig, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_by_coordesc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    990\u001B[0m ) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minductor_meta\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcoordinate_descent_tuning\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m    991\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlaunchers \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    992\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoordinate_descent_tuning(\n\u001B[1;32m    993\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlaunchers[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m*\u001B[39margs, grid\u001B[38;5;241m=\u001B[39mgrid, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    994\u001B[0m         )\n\u001B[1;32m    995\u001B[0m     ]\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/_inductor/runtime/triton_heuristics.py:870\u001B[0m, in \u001B[0;36mCachingAutotuner.autotune_to_one_config\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    868\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Do the actual autotuning\"\"\"\u001B[39;00m\n\u001B[1;32m    869\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime_ns()\n\u001B[0;32m--> 870\u001B[0m timings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbenchmark_all_configs\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    871\u001B[0m benchmark_time_taken_ns \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime_ns() \u001B[38;5;241m-\u001B[39m start_time\n\u001B[1;32m    872\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlaunchers \u001B[38;5;241m=\u001B[39m [builtins\u001B[38;5;241m.\u001B[39mmin(timings, key\u001B[38;5;241m=\u001B[39mtimings\u001B[38;5;241m.\u001B[39mget)]\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/_inductor/runtime/triton_heuristics.py:845\u001B[0m, in \u001B[0;36mCachingAutotuner.benchmark_all_configs\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    841\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbenchmark_all_configs\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    842\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m dynamo_timed(\n\u001B[1;32m    843\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCachingAutotuner.benchmark_all_configs\u001B[39m\u001B[38;5;124m\"\u001B[39m, log_pt2_compile_event\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    844\u001B[0m     ):\n\u001B[0;32m--> 845\u001B[0m         timings \u001B[38;5;241m=\u001B[39m \u001B[43m{\u001B[49m\n\u001B[1;32m    846\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlauncher\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbench\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlauncher\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    847\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mlauncher\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlaunchers\u001B[49m\n\u001B[1;32m    848\u001B[0m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\n\u001B[1;32m    850\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m timings\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    851\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoordesc_tuner\u001B[38;5;241m.\u001B[39mcache_benchmark_result(k\u001B[38;5;241m.\u001B[39mconfig, v)\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/_inductor/runtime/triton_heuristics.py:846\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    841\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbenchmark_all_configs\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    842\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m dynamo_timed(\n\u001B[1;32m    843\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCachingAutotuner.benchmark_all_configs\u001B[39m\u001B[38;5;124m\"\u001B[39m, log_pt2_compile_event\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    844\u001B[0m     ):\n\u001B[1;32m    845\u001B[0m         timings \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m--> 846\u001B[0m             launcher: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbench\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlauncher\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    847\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m launcher \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlaunchers\n\u001B[1;32m    848\u001B[0m         }\n\u001B[1;32m    850\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m timings\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    851\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoordesc_tuner\u001B[38;5;241m.\u001B[39mcache_benchmark_result(k\u001B[38;5;241m.\u001B[39mconfig, v)\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/_inductor/runtime/triton_heuristics.py:766\u001B[0m, in \u001B[0;36mCachingAutotuner.bench\u001B[0;34m(self, launcher, grid, with_profiler, *args, **kwargs)\u001B[0m\n\u001B[1;32m    763\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice_props\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    764\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m benchmarker\u001B[38;5;241m.\u001B[39mbenchmark_cpu(kernel_call)\n\u001B[0;32m--> 766\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbenchmarker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbenchmark_gpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkernel_call\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m40\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/_inductor/runtime/benchmarking.py:66\u001B[0m, in \u001B[0;36mcount.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(fn)\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;28mself\u001B[39m: Any, \u001B[38;5;241m*\u001B[39margs: P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: P\u001B[38;5;241m.\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m     63\u001B[0m     counters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minductor\u001B[39m\u001B[38;5;124m\"\u001B[39m][\n\u001B[1;32m     64\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbenchmarking.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m fn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\n\u001B[1;32m     65\u001B[0m     ] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/_inductor/runtime/benchmarking.py:201\u001B[0m, in \u001B[0;36mTritonBenchmarker.benchmark_gpu\u001B[0;34m(self, _callable, **kwargs)\u001B[0m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreturn_mode\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m kwargs:\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtriton_do_bench(_callable, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 201\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtriton_do_bench\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_callable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmedian\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/triton/testing.py:107\u001B[0m, in \u001B[0;36mdo_bench\u001B[0;34m(fn, warmup, rep, grad_to_none, quantiles, fast_flush, return_mode, device_type)\u001B[0m\n\u001B[1;32m    104\u001B[0m di \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mdevice_interface\u001B[38;5;241m.\u001B[39mget_interface_for_device(device_type)\n\u001B[1;32m    106\u001B[0m fn()\n\u001B[0;32m--> 107\u001B[0m \u001B[43mdi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msynchronize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;66;03m# We maintain a buffer of 256 MB that we clear\u001B[39;00m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;66;03m# before each kernel call to make sure that the L2\u001B[39;00m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;66;03m# doesn't contain any input data before the run\u001B[39;00m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fast_flush:\n",
      "File \u001B[0;32m/mloscratch/homes/shcherba/conda/envs/char-llm/lib/python3.11/site-packages/torch/cuda/__init__.py:969\u001B[0m, in \u001B[0;36msynchronize\u001B[0;34m(device)\u001B[0m\n\u001B[1;32m    967\u001B[0m _lazy_init()\n\u001B[1;32m    968\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mdevice(device):\n\u001B[0;32m--> 969\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cuda_synchronize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "Llama-like architecture without GQA."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T16:16:12.970238Z",
     "start_time": "2024-11-12T16:16:12.968168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LLM(nn.Module):\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training"
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters.\n",
    "\n",
    "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "\n",
    "        chars = ... # get characters from the input data\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) } # map characters to integer indices\n",
    "\n",
    "        ...\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        # encode every character to an integer\n",
    "        # return the chunk and the shifted version as tensors\n",
    "        pass"
   ],
   "metadata": {
    "id": "dZdSRWPmgt-H"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train(model, dataset, **kwargs):\n",
    "    pass"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
